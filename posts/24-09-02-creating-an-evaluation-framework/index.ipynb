{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Creating an Evaluation Framework\"\n",
        "description: \"Building out a framework that takes a model and a benchmark as input and returns the score as output.\"\n",
        "author: \"Charles Feinn\"\n",
        "date: \"9/02/2024\"\n",
        "categories:\n",
        "  - evaluations\n",
        "  - llms\n",
        "image: chart.png\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are links to the [notebook](https://github.com/chuckfinca/machine_learnings/blob/main/posts/24-09-02-creating-an-evaluation-framework/index.ipynb) and [evaluation framework](https://github.com/chuckfinca/evaluate) for this post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_AIOTbOH3Vy"
      },
      "source": [
        "## Preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-KPsQ1JIBGQ"
      },
      "source": [
        "I recently participated in the original cohort of Dan Becker and Hamel Husain's _Mastering LLMs: A Conference For Developers & Data Scientists_. I eagerly sopped up every video, hungry for lessons from the experts at the cutting edge of the field.\n",
        "\n",
        "After the course I felt I had gained a huge amount of knowledge, but I hadn't gotten my hands dirty, yet. This blog is to change that.\n",
        "\n",
        "Evaluations, which almost every speaker emphasized as being one of, if not the most, important piece of the LLM pipeline puzzle, seemed like a good place to start.\n",
        "\n",
        "For my first project, I decided to create an evaluation framework to run models on benchmarks. To simplify things I decided to focus on Meta's Llama 3.1 8B Instruct model and the Massive Multitask Language Understanding ([MMLU](https://github.com/hendrycks/test)) benchmark[^1].\n",
        "\n",
        "[^1]: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). Measuring massive multitask language understanding. arXiv. https://arxiv.org/abs/2009.03300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Intel-based Macbook Pro (2019)  \n",
        "- VSCode  \n",
        "- Google Colab  \n",
        "- Terminal (home brew theme)  \n",
        "- Jupyter Notebooks  \n",
        "  \n",
        "I also used a Claude 3.5 Sonnet Project in its web interface to assist my thoughts and code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2xLmwKjKoew"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlD9-YiVRZT9"
      },
      "source": [
        "To download our model we will use the Hugging Face Hub. This means that we will need to get and set a Hugging Face token to use in this project. Instructions for how to go about it can be found [here](https://https://huggingface.co/docs/huggingface_hub/v0.23.3/en/quick-start).\n",
        "\n",
        "Once we have our HF_TOKEN saved to our Google Colab secrets we can install and setup the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9fjwk0f2RWi6",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "268de327-165e-4dce-aee3-6bfd44e41ba8"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zWYTrYCIRn2I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Import Colab Secrets userdata module\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why MMLU?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MMLU](https://github.com/hendrycks/test), developed by Hendrycks et al. in 2020, is a comprehensive benchmark that tests LLMs on a wide range of subjects in order to determine the breadth of their general knowledge. MMLU scores are typically presented, along side other popular benchmarks, when a new model is being touted.\n",
        "\n",
        "More recently MMLU's quality has been called into question, and many practitioners believe that its test set has ended up in the training data of many models, contaminating them, and calling into question its overall usefulness.\n",
        "\n",
        "Given that, why did I choose to use it you ask?\n",
        "\n",
        "I reasoned that though MMLU might not be the best evaluation for an LLM, it was one of the original popular and influential ones, and would likely serve as a good exemplar for creating a framework that was benchmark agnostic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0BOw7HQnuNo"
      },
      "source": [
        "## The evaluation framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPnv9IoCnuNo"
      },
      "source": [
        "The framework is located at [https://github.com/chuckfinca/evaluate](https://github.com/chuckfinca/evaluate).\n",
        "\n",
        "The goal here was to create a framework that would take a model and a benchmark as input and return the model's score as output.\n",
        "\n",
        "My thinking was that if I could make it easy to run any model on any benchmark then I could use that framework in a pipeline that ran models through sets of existing and custom benchmarks alike.\n",
        "\n",
        "The framework uses an orchestrator pattern in which a benchmark specific orchestrator class (e.g. MMLUBenchmarkOrchestrator) facilitates the evaluation of a model on a benchmark. \n",
        "\n",
        "The heart of the MMLUBenchmarkOrchestrator is its subject evaluation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def _eval_subject(self, subject, dev_df, test_df):\n",
        "    cors = []\n",
        "    preds = []\n",
        "    probs = []\n",
        "\n",
        "    for i in range(len(test_df)):\n",
        "        prompt = self._format_prompt(dev_df, test_df, i)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "        \n",
        "        logits = outputs.logits[0, -1]\n",
        "        probs_i = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        \n",
        "        choice_probs = [probs_i[self.tokenizer.encode(choice, add_special_tokens=False)[0]].item() for choice in self.choices]\n",
        "        pred = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(choice_probs)]\n",
        "        \n",
        "        probs.append(choice_probs)\n",
        "        preds.append(pred)\n",
        "        cors.append(pred == test_df.iloc[i, 5])\n",
        "\n",
        "    acc = np.mean(cors)\n",
        "    print(f\"{subject} Accuracy: {acc:.3f}\")\n",
        "\n",
        "    return cors, acc, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function takes dev and test data for a given MMLU subject and evaluates the model using the following steps:\n",
        "\n",
        "1. Creates model prompts from evaluation questions and few-shot examples\n",
        "2. Tokenizes prompts and moves them to model's device\n",
        "3. Runs inference using the prompts (aka. asks the model the questions)\n",
        "4. Extracts probabilities from model outputs\n",
        "5. Predicts answers based on highest probability\n",
        "6. Calculates accuracy for the subject\n",
        "\n",
        "To see all this in action let's import the project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2knwajwpoCoP",
        "outputId": "3ebe4085-e957-4f6c-c90a-c6c323217142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'evaluate'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (261/261), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 261 (delta 150), reused 205 (delta 94), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (261/261), 36.00 KiB | 18.00 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/chuckfinca/evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's check out the right commit to make sure everything works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git checkout 24-09-02-creating-an-evaluation-framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DQoVcXSnuNp"
      },
      "source": [
        "**_NOTE_**: At some point I intend to make my evaluation framework into a python package in order to simplify some of this, but we'll save that work for a later date. \n",
        "\n",
        "The project has one dependency, so let's import that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZtIAohRnuNp",
        "outputId": "fe0e211a-c9c7-4eaa-fd46-2ee7e82e7e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NTZN-NsaQY"
      },
      "source": [
        "## Evaluating Llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9mzIhUSsdli"
      },
      "source": [
        "Now that we've got our evaluation framework ready to go we might as well spice things up! Let's see if we can reproduce [Meta's stated MMLU score](https://ai.meta.com/blog/meta-llama-3-1/#:~:text=of%20partner%20platforms.-,Model,-evaluations) of 73.0 for Llama 3.1 8B!\n",
        "\n",
        "![image](https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/452673884_1646111879501055_1352920258421649752_n.png?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=6j4zNvQHK_IQ7kNvgELs0sM&_nc_ht=scontent-sjc3-1.xx&oh=00_AYAIywC88BVcCAYMW5Tfs7zBMqJr2TQd_e7zcWaUt-rm-Q&oe=66EDDF68)\n",
        "\n",
        "I haven't yet been able to find a source in Meta's documentation that states that they used the Instruct version of their models for benchmarking, but I believe this is common practice and so I chose to use the `Meta-Llama-3.1-8B-Instruct` model in my experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running our evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run our script we just need to supply a few things. Our benchmark and model name, and the type of few-shot learning we want to use.\n",
        "\n",
        "In their blog post, Meta states that they used 0-shot (COT) to generate their 73.0 score on MMLU. To keep things uniform we will use 0 shot learning. COT (aka. chain-of-thought) is beyond the scope of this post, so we'll leave that be for now.\n",
        "\n",
        "> **_A note about hardware:_**  \n",
        ">  \n",
        "> I learned in the Mastering LLMs that a model generally requires 2 to 3x more RAM than it has billions of parameters. Our model has 8 billion parameters and so I expected to need between 16 and 24GB of GPU RAM to run the evaluation.  \n",
        ">  \n",
        "> In practice I used about 18GB. This meant that I was able to run the evaluation using both the Google Colab A100 and L4 GPU runtimes which both have 20+ GB of GPU RAM.  \n",
        ">  \n",
        "> The framework is also setup to use the CPU if cuda is not available. This worked on my local machine, but was too slow to be of any practical use.\n",
        "\n",
        "Enough of that, let's see the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOF3JrVmjv56",
        "outputId": "4e3d9f4a-89f2-4837-bd8d-96460f94ee93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmark 'mmlu' has been set up successfully.\n",
            "Running evaluation 'mmlu' with:\n",
            "Model: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "Number of training examples: 0\n",
            "Device: cuda\n",
            "Using dtype: torch.float16\n",
            "Loading model from /content/evaluate/evaluate/models/saved/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "Loading checkpoint shards: 100% 4/4 [00:00<00:00, 12.08it/s]\n",
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "abstract_algebra Accuracy: 0.340\n",
            "anatomy Accuracy: 0.607\n",
            "astronomy Accuracy: 0.757\n",
            "business_ethics Accuracy: 0.640\n",
            "clinical_knowledge Accuracy: 0.792\n",
            "college_biology Accuracy: 0.812\n",
            "college_chemistry Accuracy: 0.500\n",
            "college_computer_science Accuracy: 0.480\n",
            "college_mathematics Accuracy: 0.410\n",
            "college_medicine Accuracy: 0.699\n",
            "college_physics Accuracy: 0.412\n",
            "computer_security Accuracy: 0.740\n",
            "conceptual_physics Accuracy: 0.604\n",
            "econometrics Accuracy: 0.491\n",
            "electrical_engineering Accuracy: 0.628\n",
            "elementary_mathematics Accuracy: 0.460\n",
            "formal_logic Accuracy: 0.540\n",
            "global_facts Accuracy: 0.440\n",
            "high_school_biology Accuracy: 0.813\n",
            "high_school_chemistry Accuracy: 0.606\n",
            "high_school_computer_science Accuracy: 0.650\n",
            "high_school_european_history Accuracy: 0.739\n",
            "high_school_geography Accuracy: 0.813\n",
            "high_school_government_and_politics Accuracy: 0.850\n",
            "high_school_macroeconomics Accuracy: 0.682\n",
            "high_school_mathematics Accuracy: 0.344\n",
            "high_school_microeconomics Accuracy: 0.744\n",
            "high_school_physics Accuracy: 0.437\n",
            "high_school_psychology Accuracy: 0.881\n",
            "high_school_statistics Accuracy: 0.602\n",
            "high_school_us_history Accuracy: 0.838\n",
            "high_school_world_history Accuracy: 0.852\n",
            "human_aging Accuracy: 0.695\n",
            "human_sexuality Accuracy: 0.779\n",
            "international_law Accuracy: 0.744\n",
            "jurisprudence Accuracy: 0.778\n",
            "logical_fallacies Accuracy: 0.761\n",
            "machine_learning Accuracy: 0.509\n",
            "management Accuracy: 0.835\n",
            "marketing Accuracy: 0.880\n",
            "medical_genetics Accuracy: 0.740\n",
            "miscellaneous Accuracy: 0.834\n",
            "moral_disputes Accuracy: 0.697\n",
            "moral_scenarios Accuracy: 0.579\n",
            "nutrition Accuracy: 0.771\n",
            "philosophy Accuracy: 0.723\n",
            "prehistory Accuracy: 0.713\n",
            "professional_accounting Accuracy: 0.507\n",
            "professional_law Accuracy: 0.462\n",
            "professional_medicine Accuracy: 0.820\n",
            "professional_psychology Accuracy: 0.676\n",
            "public_relations Accuracy: 0.636\n",
            "security_studies Accuracy: 0.747\n",
            "sociology Accuracy: 0.841\n",
            "us_foreign_policy Accuracy: 0.880\n",
            "virology Accuracy: 0.536\n",
            "world_religions Accuracy: 0.813\n",
            "Score saved to: /content/evaluate/evaluate/benchmarks/benchmarks/mmlu/results/meta-llama/Meta-Llama-3.1-8B-Instruct/mmlu_score.txt\n",
            "Average accuracy: 66.600\n"
          ]
        }
      ],
      "source": [
        "!python evaluate/evaluate/main.py --benchmark mmlu --model meta-llama/Meta-Llama-3.1-8B-Instruct --nshot 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60x0QP-8Cfhu"
      },
      "source": [
        "**Et voila!** We have our score.\n",
        "\n",
        "Not what Meta reported, but not that far off either. I'll leave the discrepancy for future posts :)\n",
        "\n",
        "Here are links to the [notebook](https://github.com/chuckfinca/machine_learnings/blob/main/posts/24-09-02-creating-an-evaluation-framework/index.ipynb) and [evaluation framework](https://github.com/chuckfinca/evaluate) for this post. Now go play!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thanks for following along. If you've got a questions or comment please feel free to email me at `chuckfinca at gmail dot com`."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
