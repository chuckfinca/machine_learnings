[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I recently participated in the original cohort of Dan Becker and Hamel Husain’s Mastering LLMs: A Conference For Developers & Data Scientists. I eagerly sopped up every video, hungry for lessons from the experts at the cutting edge of the field.\nAfter the course I felt I had gained a huge amount of knowledge, but I hadn’t gotten my hands dirty, yet.\nThis blog is to change that."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "",
    "text": "In my previous post, Creating an Evaluation Framework, I created a simple evaluation framework for LLMs. I attempted to reproduce Meta’s Llama 3.1 8B Massive Multitask Language Understanding (MMLU)1 macro score of 73.02. TLDR: I couldn’t quite get there, but I did journey pretty deep into the messy world of LLM evaluation."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#introduction",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#introduction",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "",
    "text": "In my previous post, Creating an Evaluation Framework, I created a simple evaluation framework for LLMs. I attempted to reproduce Meta’s Llama 3.1 8B Massive Multitask Language Understanding (MMLU)1 macro score of 73.02. TLDR: I couldn’t quite get there, but I did journey pretty deep into the messy world of LLM evaluation."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#the-reproduction-challenge",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#the-reproduction-challenge",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "The Reproduction Challenge",
    "text": "The Reproduction Challenge\nI initially achieved a micro score of 66.6 and a macro score of 67.03. That’s a 6.0 point gap from Meta’s published score. The post focused on building out the evaluation framework so I didn’t spend time investigating, but it left me wondering.\n6.0 points in MMLU is a lot. It represents almost half the difference between Meta’s 8B and 70B models (73.0 and 86.0 respectively).\nWhen thinking about what could have caused this difference I remembered something that Hamel Husain said in Mastering LLMs: A Conference For Developers & Data Scientists. The gist was that you should never trust what is going in to your model. You need to look at it, understand any template that is being applied, and verify that is what you want. He even wrote a rather provocative blog post on the subject4."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#looking-under-the-hood",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#looking-under-the-hood",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "Looking under the hood",
    "text": "Looking under the hood\nTruth be told, I had Claude handle this part from my previous post. It had seemed to work, and wasn’t the focus of the post, so I hadn’t delved deeper. Now was the time to change that.\nSo, first things first, let’s look under the hood5:\n\ndef _format_prompt(self, dev_df, test_df, test_idx):\n    prompt = \"Answer the following multiple choice questions. Choose the best answer from A, B, C, or D.\\n\\n\"\n    for i in range(len(dev_df)):\n        prompt += self._format_example(dev_df, i) + \"\\n\\n\"\n    prompt += self._format_example(test_df, test_idx, include_answer=False)\n    return prompt\n\ndef _format_example(self, df, idx, include_answer=True):\n    prompt = df.iloc[idx, 0]\n    for j, choice in enumerate(self.choices):\n        prompt += f\"\\n{choice}. {df.iloc[idx, j+1]}\"\n    prompt += \"\\nAnswer:\"\n    if include_answer:\n        prompt += f\" {df.iloc[idx, 5]}\"\n    return prompt\n\nIt is very difficult to visualize the prompt from these functions. It is clear that there are various parts (instructions, examples, test, etc.), and that they are combined using new lines in some way, but the specifics are hard to grok from the code.\nSo I went about refactoring so that the prompt was configured in the config which informed the model:\nbasic_config.json:\n\n{\n    \"benchmark_name\": \"mmlu\",\n    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"nshot\": 0,\n    \"answer_choices\": [\"A\", \"B\", \"C\", \"D\"],\n    \"use_chat_template\": false,\n    \"system_prompt\": \"None\",\n    \"user_prompt_template\": {\n        \"template\": \"{instructions}\\n\\n{question}\",\n        \"instructions\": \"Answer the following multiple choice questions. Choose the best answer from {label_a}, {label_b}, {label_c}, or {label_d}.\",\n        \"question_template\": \"{question}\\n{label_a}. {choice_a}\\n{label_b}. {choice_b}\\n{label_c}. {choice_c}\\n{label_d}. {choice_d}\\n\\nAnswer: \",\n        \"question_separator\": \"\\n\\n\"\n    },\n    \"log_level\": \"INFO\",\n    \"cap_subjects\" : false,\n    \"generation_type\": \"inference\"\n}\n\nThe basic_config.json results in a prompt with format:\n\nThe following are multiple choice questions (with answers) about {subject}.\n{question}\nA. {choice_a}\nB. {choice_b}\nC. {choice_c}\nD. {choice_d}\nAnswer:\n\nUsing this setup I was able to get a micro average of 66.76, which is just a hair better than my score of 66.6 from the original post.\nWith my baseline set, I went about trying to reproduce Meta’s reported 73.07."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#down-the-rabbit-hole-of-evaluation-methods",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#down-the-rabbit-hole-of-evaluation-methods",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "Down the Rabbit Hole of Evaluation Methods",
    "text": "Down the Rabbit Hole of Evaluation Methods\n\nThe Many Faces of Evaluation\nThere are a lot of ways that this evaluation can be performed. A few possibilities include:\n1) Let the model complete its text response and extract the answer (a.k.a. open-ended generation)\n2) Look at the logits after a single round of inference and pick the most probable answer\n3) Constrain the logit choices to valid answers and pick the most probable (a.k.a. constrained decoding)\nEach method has its pros and cons, involving trade-offs between computation cost, potential for the model to go off the rails, and the hassle of extracting answers from free-form text.\nIn Meta’s supplemental Llama 3 Evaluation Details they say that “the maximum generation lengths for the 5-shot and 0-shot configs are 10 tokens and 1024 tokens respectively”8. This leads me to believe that they used open-ended generation, which contrasted with the constrained decoding which I had been doing.\n\n\nThe Quest for the Perfect Prompt\nI experimented with N-shot, Chain-of-Thought (CoT), and chat templates. I experimented with open-ended generation, looking at logits, and constrained decoding. For example, I tried using open-ended generation with the 0-shot chat template used in Sprague et al.’s To CoT or not to CoT (2024)9:\nchat_template_config.json\n\n{\n    \"benchmark_name\": \"mmlu\",\n    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"nshot\": 0,\n    \"answer_choices\": [\"A\", \"B\", \"C\", \"D\"],\n    \"use_chat_template\": true,\n    \"system_prompt\": \"You answer questions. At the end of the question you always give an answer and nothing else. You must pick an answer. You always give only one answer and that one answer is the one you think is best. You always give the answer in the form of the answer choice letter.\",\n    \"user_prompt_template\": {\n        \"template\": \"{instructions}\\n{question}\",\n        \"question_template\": \"{question}\\n{label_a}. {choice_a}\\n{label_b}. {choice_b}\\n{label_c}. {choice_c}\\n{label_d}. {choice_d}\\n\\nAnswer: \",\n        \"question_separator\": \"\\n\\n\",\n        \"instructions\": \"Give your answer in the format \\\"The answer is therefore &lt;{label_a}, {label_b}, {label_c}, {label_d}&gt;\\\". Failure to comply with the answer formatting will result in no credit.\"\n    }\n}\n\nTo create a prompt that looked like:\n\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\nYou answer questions. At the end of the question you always give an answer and nothing else. You must pick an answer. You always give only one answer and that one answer is the one you think is best. You always give the answer in the form of the answer choice letter.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\nGive your answer in the format \"The answer is therefore &lt;A, B, C, D&gt;\". Failure to comply with the answer formatting will result in no credit.\n{question} A. {choice_a}\nB. {choice_b}\nC. {choice_c}\nD. {choice_d}\nAnswer:&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\nResult10? A measly 61.0. Back to the drawing board.\n\n\nThe original MMLU prompt\nTurning back to Meta’s documentation I decided to look at the language used in my prompt. Meta stated that it had used the original MMLU prompt11, so I sought it out.\nAccording to What’s going on with the Open LLM Leaderboard?, the standard MMLU prompt is12:\n\nThe following are multiple choice questions (with answers) about {subject}.\n{question}\nA. {choice_a}\nB. {choice_b}\nC. {choice_c}\nD. {choice_d}\nAnswer:\n\nTo reproduce the prompt I created a new config:\noriginal_mmlu_config.json\n\n{\n    \"benchmark_name\": \"mmlu\",\n    \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"nshot\": 0,\n    \"answer_choices\": [\"A\", \"B\", \"C\", \"D\"],\n    \"use_chat_template\": false,\n    \"system_prompt\": \"\",\n    \"user_prompt_template\": {\n        \"template\": \"{instructions}\\n{question}\",\n        \"instructions\": \"The following are multiple choice questions (with answers) about {subject}.\",\n        \"question_template\": \"{question}\\n{label_a}. {choice_a}\\n{label_b}. {choice_b}\\n{label_c}. {choice_c}\\n{label_d}. {choice_d}\\nAnswer: \",\n        \"question_separator\": \"\\n\\n\"\n    },\n    \"log_level\": \"INFO\",\n    \"cap_subjects\" : false,\n    \"generation_type\": \"inference\"\n}\n\nThis simple approach yielded my best score yet: 68.313."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#wrapping-my-head-around-it-all",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#wrapping-my-head-around-it-all",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "Wrapping My Head Around It All",
    "text": "Wrapping My Head Around It All\n\nThe Prompt Sensitivity Conundrum\nThroughout this journey, I learned that these models are surprisingly sensitive to prompts14. Tiny changes can lead to big swings in performance. What works on one model doesn’t work on another. It’s definitely more art than science.\n\n\nThe Evaluation Quagmire\nThe more I dug into this, the messier it got. Reproducibility issues, stochastic weirdness, and an uncomfortable reliance on prompt engineering made me question the whole evaluation game. Are we really measuring model capability, or just our ability to craft the perfect prompt?\nDon’t even get me started on training on the test set.\n\n\nA Ray of Hope\nIt’s not all doom and gloom, though. Third-party evaluators like HuggingFace’s Open LLM Leaderboard are doing the community a huge service by providing free, consistent, reproducible benchmarks allowing for relative comparisons between models. And ultimately evaluations are only a tool. The most important evaluations will be the custom ones that show you have the right model for your task."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#concluding-thoughts",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#concluding-thoughts",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nThis deep dive into MMLU evaluation left me with more questions than answers. But isn’t that how all good scientific inquiries go? We’ve got a long way to go in standardizing LLM evaluation, but finding a way to objectively assess these stochastic machines is incredibly important.\nSo, the next time you see a flashy headline about some model’s incredible benchmark performance, remember - there’s probably a lot of prompt engineering magic happening behind the curtain. And reproducing those results? Well, that’s a whole other can of worms.\nUntil next time."
  },
  {
    "objectID": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#footnotes",
    "href": "posts/24-10-03-challenges-in-reproducing-llm-evaluation-results/index.html#footnotes",
    "title": "Challenges in Reproducing LLM Evaluation Results",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). Measuring massive multitask language understanding. arXiv. https://arxiv.org/abs/2009.03300↩︎\nMeta. (2024, July 23). Introducing Llama 3.1: Our most capable models to date. Meta AI. https://ai.meta.com/blog/meta-llama-3-1/↩︎\nNOTE:\nThe macro average is the average of each subject’s score.\nThe micro average is calculated by aggregating all subjects responses.\nMeta’s reported score of 73.0 was a macro average score. My previous post reported the micro average.\nIn the rest of this post I will refer to the macro score unless otherwise noted.↩︎\nHusain, H. (2024, February 14). Fuck You, Show Me The Prompt. hamel.dev. https://hamel.dev/blog/posts/prompt/↩︎\nTo reproduce:\ngit clone https://github.com/chuckfinca/evaluate.git\ncd evaluate  \ngit checkout 24-09-02-creating-an-evaluation-framework\n↩︎\nTo reproduce:\ngit clone https://github.com/chuckfinca/evaluate.git\ncd evaluate  \ngit checkout 3203ed4\npython evaluate/evaluate/main.py basic_config.json\n↩︎\nMeta. (2024, July 23). Introducing Llama 3.1: Our most capable models to date. Meta AI. https://ai.meta.com/blog/meta-llama-3-1/↩︎\nMeta Llama. (2024, July 23). Llama 3 Evaluation Details [Computer software]. GitHub. https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md↩︎\nSprague, Z., Yin, F., Rodriguez, J. D., Jiang, D., Wadhwa, M., Singhal, P., Zhao, X., Ye, X., Mahowald, K., & Durrett, G. (2024). To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning. arXiv. https://doi.org/10.48550/arXiv.2409.12183↩︎\nTo reproduce:\ngit clone https://github.com/chuckfinca/evaluate.git\ncd evaluate  \ngit checkout 3203ed4\npython evaluate/evaluate/main.py chat_template_config.json\n↩︎\nMeta Llama. (2024, July 23). Llama 3 Evaluation Details [Computer software]. GitHub. https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md↩︎\nFourrier, C., Habib, N., Launay, J., Wolf, T. (2023, June 23). What’s Going on with the Open LLM Leaderboard. Hugging Face Blog. https://huggingface.co/blog/open-llm-leaderboard-mmlu↩︎\nTo reproduce:\ngit clone https://github.com/chuckfinca/evaluate.git\ncd evaluate  \ngit checkout 3203ed4\npython evaluate/evaluate/main.py original_mmlu_config.json\n↩︎\nSchulhoff, S., Ilie, M., Balepur, N., Kahadze, K., Liu, A., Si, C., Li, Y., Gupta, A., Han, H., Schulhoff, S., Dulepet, P. S., Vidyadhara, S., Ki, D., Agrawal, S., Pham, C., Kroiz, G., Li, F., Tao, H., Srivastava, A., Da Costa, H., Gupta, S., Rogers, M. L., Goncearenco, I., Sarli, G., Galynker, I., Peskoff, D., Carpuat, M., White, J., Anadkat, S., Hoyle, A., & Resnik, P. (2024). The Prompt Report: A Systematic Survey of Prompting Techniques. arXiv. https://arxiv.org/abs/2406.06608v3↩︎"
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html",
    "title": "Creating an Evaluation Framework",
    "section": "",
    "text": "Here are links to the notebook and evaluation framework for this post. You can run the project in Google Colab using a L4 runtime."
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#preface",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#preface",
    "title": "Creating an Evaluation Framework",
    "section": "Preface",
    "text": "Preface\nI recently participated in the original cohort of Dan Becker and Hamel Husain’s Mastering LLMs: A Conference For Developers & Data Scientists. I eagerly sopped up every video, hungry for lessons from the experts at the cutting edge of the field.\nAfter the course I felt I had gained a huge amount of knowledge, but I hadn’t gotten my hands dirty, yet. This blog is to change that.\nEvaluations, which almost every speaker emphasized as being one of, if not the most, important piece of the LLM pipeline puzzle, seemed like a good place to start.\nFor my first project, I decided to create an evaluation framework to run models on benchmarks. To simplify things I decided to focus on Meta’s Llama 3.1 8B Instruct model and the Massive Multitask Language Understanding (MMLU) benchmark1."
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#gear",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#gear",
    "title": "Creating an Evaluation Framework",
    "section": "Gear",
    "text": "Gear\n\nIntel-based Macbook Pro (2019)\n\nVSCode\n\nGoogle Colab\n\nTerminal (home brew theme)\n\nJupyter Notebooks\n\nI also used a Claude 3.5 Sonnet Project in its web interface to assist my thoughts and code."
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#setup",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#setup",
    "title": "Creating an Evaluation Framework",
    "section": "Setup",
    "text": "Setup\nTo download our model we will use the Hugging Face Hub. This means that we will need to get and set a Hugging Face token to use in this project. Instructions for how to go about it can be found here.\nOnce we have our HF_TOKEN saved to our Google Colab secrets we can install and setup the Hugging Face Hub.\n\n!pip install huggingface_hub\n\n\nimport os\n\n# Import Colab Secrets userdata module\nfrom google.colab import userdata\n\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#why-mmlu",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#why-mmlu",
    "title": "Creating an Evaluation Framework",
    "section": "Why MMLU?",
    "text": "Why MMLU?\nMMLU, developed by Hendrycks et al. in 2020, is a comprehensive benchmark that tests LLMs on a wide range of subjects in order to determine the breadth of their general knowledge. MMLU scores are typically presented, along side other popular benchmarks, when a new model is being touted.\nMore recently MMLU’s quality has been called into question, and many practitioners believe that its test set has ended up in the training data of many models, contaminating them, and calling into question its overall usefulness.\nGiven that, why did I choose to use it you ask?\nI reasoned that though MMLU might not be the best evaluation for an LLM, it was one of the original popular and influential ones, and would likely serve as a good exemplar for creating a framework that was benchmark agnostic."
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#the-evaluation-framework",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#the-evaluation-framework",
    "title": "Creating an Evaluation Framework",
    "section": "The evaluation framework",
    "text": "The evaluation framework\nThe framework is located at https://github.com/chuckfinca/evaluate.\nThe goal here was to create a framework that would take a model and a benchmark as input and return the model’s score as output.\nMy thinking was that if I could make it easy to run any model on any benchmark then I could use that framework in a pipeline that ran models through sets of existing and custom benchmarks alike.\nThe framework uses an orchestrator pattern in which a benchmark specific orchestrator class (e.g. MMLUBenchmarkOrchestrator) facilitates the evaluation of a model on a benchmark.\nThe heart of the MMLUBenchmarkOrchestrator is its subject evaluation function:\n\nimport torch\nimport numpy as np\n\ndef _eval_subject(self, subject, dev_df, test_df):\n    cors = []\n    preds = []\n    probs = []\n\n    for i in range(len(test_df)):\n        prompt = self._format_prompt(dev_df, test_df, i)\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        \n        logits = outputs.logits[0, -1]\n        probs_i = torch.nn.functional.softmax(logits, dim=-1)\n        \n        choice_probs = [probs_i[self.tokenizer.encode(choice, add_special_tokens=False)[0]].item() for choice in self.choices]\n        pred = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(choice_probs)]\n        \n        probs.append(choice_probs)\n        preds.append(pred)\n        cors.append(pred == test_df.iloc[i, 5])\n\n    acc = np.mean(cors)\n    print(f\"{subject} Accuracy: {acc:.3f}\")\n\n    return cors, acc, probs\n\nThe function takes dev and test data for a given MMLU subject and evaluates the model using the following steps:\n\nCreates model prompts from evaluation questions and few-shot examples\nTokenizes prompts and moves them to model’s device\nRuns inference using the prompts (aka. asks the model the questions)\nExtracts probabilities from model outputs\nPredicts answers based on highest probability\nCalculates accuracy for the subject\n\nTo see all this in action let’s import the project (at the appropriate commit tag 24-09-02-creating-an-evaluation-framework):\n\n!git clone --branch 24-09-02-creating-an-evaluation-framework https://github.com/chuckfinca/evaluate \n\nCloning into 'evaluate'...\nremote: Enumerating objects: 261, done.\nremote: Counting objects: 100% (261/261), done.\nremote: Compressing objects: 100% (149/149), done.\nremote: Total 261 (delta 150), reused 205 (delta 94), pack-reused 0 (from 0)\nReceiving objects: 100% (261/261), 36.00 KiB | 18.00 MiB/s, done.\nResolving deltas: 100% (150/150), done.\n\n\nNOTE: At some point I intend to make my evaluation framework into a python package in order to simplify some of this, but we’ll save that work for a later date.\nThe project has one dependency, so let’s import that:\n\n!pip install python-dotenv\n\nCollecting python-dotenv\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.0.1"
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#evaluating-llama",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#evaluating-llama",
    "title": "Creating an Evaluation Framework",
    "section": "Evaluating Llama",
    "text": "Evaluating Llama\nNow that we’ve got our evaluation framework ready to go we might as well spice things up! Let’s see if we can reproduce Meta’s stated MMLU score of 73.0 for Llama 3.1 8B!\n\n\n\nimage\n\n\nI haven’t yet been able to find a source in Meta’s documentation that states that they used the Instruct version of their models for benchmarking, but I believe this is common practice and so I chose to use the Meta-Llama-3.1-8B-Instruct model in my experiment."
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#running-our-evaluation",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#running-our-evaluation",
    "title": "Creating an Evaluation Framework",
    "section": "Running our evaluation",
    "text": "Running our evaluation\nTo run our script we just need to supply a few things. Our benchmark and model name, and the type of few-shot learning we want to use.\nIn their blog post, Meta states that they used 0-shot (COT) to generate their 73.0 score on MMLU. To keep things uniform we will use 0 shot learning. COT (aka. chain-of-thought) is beyond the scope of this post, so we’ll leave that be for now.\n\nA note about hardware:\nI learned in the Mastering LLMs that a model generally requires 2 to 3x more RAM than it has billions of parameters. Our model has 8 billion parameters and so I expected to need between 16 and 24GB of GPU RAM to run the evaluation.\nIn practice I used about 18GB. This meant that I was able to run the evaluation using both the Google Colab A100 and L4 GPU runtimes which both have 20+ GB of GPU RAM.\nThe framework is also setup to use the CPU if cuda is not available. This worked on my local machine, but was too slow to be of any practical use.\n\nEnough of that, let’s see the results!\n\n!python evaluate/evaluate/main.py --benchmark mmlu --model meta-llama/Meta-Llama-3.1-8B-Instruct --nshot 0\n\nBenchmark 'mmlu' has been set up successfully.\nRunning evaluation 'mmlu' with:\nModel: meta-llama/Meta-Llama-3.1-8B-Instruct\nNumber of training examples: 0\nDevice: cuda\nUsing dtype: torch.float16\nLoading model from /content/evaluate/evaluate/models/saved/meta-llama/Meta-Llama-3.1-8B-Instruct\nLoading checkpoint shards: 100% 4/4 [00:00&lt;00:00, 12.08it/s]\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\nabstract_algebra Accuracy: 0.340\nanatomy Accuracy: 0.607\nastronomy Accuracy: 0.757\nbusiness_ethics Accuracy: 0.640\nclinical_knowledge Accuracy: 0.792\ncollege_biology Accuracy: 0.812\ncollege_chemistry Accuracy: 0.500\ncollege_computer_science Accuracy: 0.480\ncollege_mathematics Accuracy: 0.410\ncollege_medicine Accuracy: 0.699\ncollege_physics Accuracy: 0.412\ncomputer_security Accuracy: 0.740\nconceptual_physics Accuracy: 0.604\neconometrics Accuracy: 0.491\nelectrical_engineering Accuracy: 0.628\nelementary_mathematics Accuracy: 0.460\nformal_logic Accuracy: 0.540\nglobal_facts Accuracy: 0.440\nhigh_school_biology Accuracy: 0.813\nhigh_school_chemistry Accuracy: 0.606\nhigh_school_computer_science Accuracy: 0.650\nhigh_school_european_history Accuracy: 0.739\nhigh_school_geography Accuracy: 0.813\nhigh_school_government_and_politics Accuracy: 0.850\nhigh_school_macroeconomics Accuracy: 0.682\nhigh_school_mathematics Accuracy: 0.344\nhigh_school_microeconomics Accuracy: 0.744\nhigh_school_physics Accuracy: 0.437\nhigh_school_psychology Accuracy: 0.881\nhigh_school_statistics Accuracy: 0.602\nhigh_school_us_history Accuracy: 0.838\nhigh_school_world_history Accuracy: 0.852\nhuman_aging Accuracy: 0.695\nhuman_sexuality Accuracy: 0.779\ninternational_law Accuracy: 0.744\njurisprudence Accuracy: 0.778\nlogical_fallacies Accuracy: 0.761\nmachine_learning Accuracy: 0.509\nmanagement Accuracy: 0.835\nmarketing Accuracy: 0.880\nmedical_genetics Accuracy: 0.740\nmiscellaneous Accuracy: 0.834\nmoral_disputes Accuracy: 0.697\nmoral_scenarios Accuracy: 0.579\nnutrition Accuracy: 0.771\nphilosophy Accuracy: 0.723\nprehistory Accuracy: 0.713\nprofessional_accounting Accuracy: 0.507\nprofessional_law Accuracy: 0.462\nprofessional_medicine Accuracy: 0.820\nprofessional_psychology Accuracy: 0.676\npublic_relations Accuracy: 0.636\nsecurity_studies Accuracy: 0.747\nsociology Accuracy: 0.841\nus_foreign_policy Accuracy: 0.880\nvirology Accuracy: 0.536\nworld_religions Accuracy: 0.813\nScore saved to: /content/evaluate/evaluate/benchmarks/benchmarks/mmlu/results/meta-llama/Meta-Llama-3.1-8B-Instruct/mmlu_score.txt\nAverage accuracy: 66.600\n\n\nEt voila! We have our score.\nNot what Meta reported, but not that far off either. I’ll leave the discrepancy for future posts :)\nHere are links to the notebook and evaluation framework for this post. Now go play!\nThanks for following along. If you’ve got a questions or comment please feel free to email me at chuckfinca at gmail dot com."
  },
  {
    "objectID": "posts/24-09-02-creating-an-evaluation-framework/index.html#footnotes",
    "href": "posts/24-09-02-creating-an-evaluation-framework/index.html#footnotes",
    "title": "Creating an Evaluation Framework",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2021). Measuring massive multitask language understanding. arXiv. https://arxiv.org/abs/2009.03300↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learnings",
    "section": "",
    "text": "Challenges in Reproducing LLM Evaluation Results\n\n\n\n\n\n\nevaluations\n\n\nllms\n\n\nprompts\n\n\nmmlu\n\n\n\nA Case Study with MMLU and Llama 3\n\n\n\n\n\nOct 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an Evaluation Framework\n\n\n\n\n\n\nevaluations\n\n\nllms\n\n\n\nBuilding out a framework that takes a model and a benchmark as input and returns the score as output.\n\n\n\n\n\nSep 2, 2024\n\n\n\n\n\n\nNo matching items"
  }
]