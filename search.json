[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I recently participated in the original cohort of Dan Becker and Hamel Husain’s Mastering LLMs: A Conference For Developers & Data Scientists. I eagerly sopped up every video, hungry for lessons from the experts at the cutting edge of the field.\nAfter the course I felt I had gained a huge amount of knowledge, but I hadn’t gotten my hands dirty, yet.\nThis blog is to change that."
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html",
    "title": "Creating an Evaluation Framework",
    "section": "",
    "text": "Here are links to the notebook and evaluation framework for this post."
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html#preface",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html#preface",
    "title": "Creating an Evaluation Framework",
    "section": "Preface",
    "text": "Preface\nI recently participated in the original cohort of Dan Becker and Hamel Husain’s Mastering LLMs: A Conference For Developers & Data Scientists. I eagerly sopped up every video, hungry for lessons from the experts at the cutting edge of the field.\nAfter the course I felt I had gained a huge amount of knowledge, but I hadn’t gotten my hands dirty, yet. This blog is to change that.\nEvaluations, which almost every speaker emphasized as being one of, if not the most, important piece of the LLM pipeline puzzle, seemed like a good place to start.\nFor my first project, I decided to create an evaluation framework to run models on benchmarks. To simplify things I decided to focus on Meta’s Llama 3.1 8B Instruct model and the Massive Multitask Language Understanding (MMLU) benchmark."
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html#gear",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html#gear",
    "title": "Creating an Evaluation Framework",
    "section": "Gear",
    "text": "Gear\n\nIntel-based Macbook Pro (2019)\n\nVSCode\n\nGoogle Colab\n\nTerminal (home brew theme)\n\nJupyter Notebooks\n\nI also used a Claude 3.5 Sonnet Project in its web interface to assist my thoughts and code."
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html#setup",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html#setup",
    "title": "Creating an Evaluation Framework",
    "section": "Setup",
    "text": "Setup\nTo download our model we will use the Hugging Face Hub. This means that we will need to get and set a Hugging Face token to use in this project. Instructions for how to go about it can be found here.\nOnce we have our HF_TOKEN saved to our Google Colab secrets we can install and setup the Hugging Face Hub.\n\n!pip install huggingface_hub\n\n\nimport os\n\n# Import Colab Secrets userdata module\nfrom google.colab import userdata\n\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html#why-mmlu",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html#why-mmlu",
    "title": "Creating an Evaluation Framework",
    "section": "Why MMLU?",
    "text": "Why MMLU?\nMMLU is a often used benchmark that tests LLMs on a wide range of subjects in order to determine the breadth of their general knowledge. MMLU scores are typically presented, along side other popular benchmarks, when a new model is being touted.\nMore recently MMLU’s quality has been called into question, and many practicioners believe that its test set has ended up in the training data of many models, contaminating them, and calling into question its overall usefulness.\nGiven that, why did I choose to use it you ask?\nI reasoned that though MMLU might not be the best evaluation for an LLM, it was one of the original popular and influencial ones, and would likely serve as a good exemplar for creating a framework that was benchmark agnostic."
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html#the-evaluation-framework",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html#the-evaluation-framework",
    "title": "Creating an Evaluation Framework",
    "section": "The evaluation framework",
    "text": "The evaluation framework\nThe framework is located at https://github.com/chuckfinca/evaluate.\nThe goal here was to create a framework that would take a model and a benchmark as input and return the model’s score as output.\nMy thinking was that if I could make it easy to run any model on any benchmark then I could use that framework in a pipeline that ran models through sets of existing and custom benchmarks alike.\nThe framework uses an orchestrator pattern in which a benchmark specific orchestrator class (e.g. MMLUBenchmarkOrchestrator) facilitates the evaluation of a model on a benchmark.\nThe heart of the MMLUBenchmarkOrchestrator is its subject evaluation function:\n\nimport torch\nimport numpy as np\n\ndef _eval_subject(self, subject, dev_df, test_df):\n    cors = []\n    preds = []\n    probs = []\n\n    for i in range(len(test_df)):\n        prompt = self._format_prompt(dev_df, test_df, i)\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        \n        logits = outputs.logits[0, -1]\n        probs_i = torch.nn.functional.softmax(logits, dim=-1)\n        \n        choice_probs = [probs_i[self.tokenizer.encode(choice, add_special_tokens=False)[0]].item() for choice in self.choices]\n        pred = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(choice_probs)]\n        \n        probs.append(choice_probs)\n        preds.append(pred)\n        cors.append(pred == test_df.iloc[i, 5])\n\n    acc = np.mean(cors)\n    print(f\"{subject} Accuracy: {acc:.3f}\")\n\n    return cors, acc, probs\n\nThe function takes dev and test data for a given MMLU subject and evaluates the model using the following steps:\n\nCreates model prompts from evaluation questions and few-shot examples\nTokenizes prompts and moves them to model’s device\nRuns inference using the prompts (aka. asks the model the questions)\nExtracts probabilities from model outputs\nPredicts answers based on highest probability\nCalculates accuracy for the subject\n\nTo see all this in action let’s import the project now:\n\n!git clone https://github.com/chuckfinca/evaluate\n\nCloning into 'evaluate'...\nremote: Enumerating objects: 261, done.\nremote: Counting objects: 100% (261/261), done.\nremote: Compressing objects: 100% (149/149), done.\nremote: Total 261 (delta 150), reused 205 (delta 94), pack-reused 0 (from 0)\nReceiving objects: 100% (261/261), 36.00 KiB | 18.00 MiB/s, done.\nResolving deltas: 100% (150/150), done.\n\n\nThe project has one dependency, so let’s import that:\n\n!pip install python-dotenv\n\nCollecting python-dotenv\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.0.1\n\n\nNOTE: At some point I intend to make my evaluation framework into a python package in order to simplify some of this, but we’ll save that work for a later date."
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html#evaluating-llama",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html#evaluating-llama",
    "title": "Creating an Evaluation Framework",
    "section": "Evaluating Llama",
    "text": "Evaluating Llama\nNow that we’ve got our evaluation framework ready to go we might as well spice things up! Let’s see if we can reproduce Meta’s stated MMLU score of 73.0 for Llama 3.1 8B!\n\n\n\nimage\n\n\nI haven’t yet been able to find a source in Meta’s documentation that states that they used the Instruct version of their models for benchmarking, but I believe this is common practice and so I chose to use the Meta-Llama-3.1-8B-Instruct model in my experiment."
  },
  {
    "objectID": "posts/24-08-31-creating-an-evaluation-framework/index.html#running-our-evaluation",
    "href": "posts/24-08-31-creating-an-evaluation-framework/index.html#running-our-evaluation",
    "title": "Creating an Evaluation Framework",
    "section": "Running our evaluation",
    "text": "Running our evaluation\nTo run our script we just need to supply a few things. Our benchmark and model name, and the type of few-shot learning we want to use.\nIn their blog post, Meta states that they used 0-shot (COT) to generate their 73.0 score on MMLU. To keep things uniform we will use 0 shot learning. COT (aka. chain-of-thought) is beyond the scope of this post, so we’ll leave that be for now.\n\nA note about hardware:\nI learned in the Mastering LLMs that a model generally requires 2 to 3x more RAM than it has billions of parameters. Our model has 8 billion parameters and so I expected to need between 16 and 24GB of GPU RAM to run the evaluation.\nIn practice I used about 18GB. This meant that I was able to run the evaluation using both the Google Colab A100 and L4 GPU runtimes which both have 20+ GB of GPU RAM.\nThe framework is also setup to use the CPU if cuda is not available. This worked on my local machine, but was too slow to be of any practical use.\n\nEnough of that, let’s see the results!\n\n!python evaluate/evaluate/main.py --benchmark mmlu --model meta-llama/Meta-Llama-3.1-8B-Instruct --nshot 0\n\nBenchmark 'mmlu' has been set up successfully.\nRunning evaluation 'mmlu' with:\nModel: meta-llama/Meta-Llama-3.1-8B-Instruct\nNumber of training examples: 0\nDevice: cuda\nUsing dtype: torch.float16\nLoading model from /content/evaluate/evaluate/models/saved/meta-llama/Meta-Llama-3.1-8B-Instruct\nLoading checkpoint shards: 100% 4/4 [00:00&lt;00:00, 12.08it/s]\nWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\nabstract_algebra Accuracy: 0.340\nanatomy Accuracy: 0.607\nastronomy Accuracy: 0.757\nbusiness_ethics Accuracy: 0.640\nclinical_knowledge Accuracy: 0.792\ncollege_biology Accuracy: 0.812\ncollege_chemistry Accuracy: 0.500\ncollege_computer_science Accuracy: 0.480\ncollege_mathematics Accuracy: 0.410\ncollege_medicine Accuracy: 0.699\ncollege_physics Accuracy: 0.412\ncomputer_security Accuracy: 0.740\nconceptual_physics Accuracy: 0.604\neconometrics Accuracy: 0.491\nelectrical_engineering Accuracy: 0.628\nelementary_mathematics Accuracy: 0.460\nformal_logic Accuracy: 0.540\nglobal_facts Accuracy: 0.440\nhigh_school_biology Accuracy: 0.813\nhigh_school_chemistry Accuracy: 0.606\nhigh_school_computer_science Accuracy: 0.650\nhigh_school_european_history Accuracy: 0.739\nhigh_school_geography Accuracy: 0.813\nhigh_school_government_and_politics Accuracy: 0.850\nhigh_school_macroeconomics Accuracy: 0.682\nhigh_school_mathematics Accuracy: 0.344\nhigh_school_microeconomics Accuracy: 0.744\nhigh_school_physics Accuracy: 0.437\nhigh_school_psychology Accuracy: 0.881\nhigh_school_statistics Accuracy: 0.602\nhigh_school_us_history Accuracy: 0.838\nhigh_school_world_history Accuracy: 0.852\nhuman_aging Accuracy: 0.695\nhuman_sexuality Accuracy: 0.779\ninternational_law Accuracy: 0.744\njurisprudence Accuracy: 0.778\nlogical_fallacies Accuracy: 0.761\nmachine_learning Accuracy: 0.509\nmanagement Accuracy: 0.835\nmarketing Accuracy: 0.880\nmedical_genetics Accuracy: 0.740\nmiscellaneous Accuracy: 0.834\nmoral_disputes Accuracy: 0.697\nmoral_scenarios Accuracy: 0.579\nnutrition Accuracy: 0.771\nphilosophy Accuracy: 0.723\nprehistory Accuracy: 0.713\nprofessional_accounting Accuracy: 0.507\nprofessional_law Accuracy: 0.462\nprofessional_medicine Accuracy: 0.820\nprofessional_psychology Accuracy: 0.676\npublic_relations Accuracy: 0.636\nsecurity_studies Accuracy: 0.747\nsociology Accuracy: 0.841\nus_foreign_policy Accuracy: 0.880\nvirology Accuracy: 0.536\nworld_religions Accuracy: 0.813\nScore saved to: /content/evaluate/evaluate/benchmarks/benchmarks/mmlu/results/meta-llama/Meta-Llama-3.1-8B-Instruct/mmlu_score.txt\nAverage accuracy: 66.600\n\n\nEt voila! We have our score.\nNot what Meta reported, but not that far off either. I’ll leave the discrepancy for future posts :)\nHere are links to the notebook and evaluation framework for this post. Now go play!\nThanks for following along. If you’ve got a questions or comment please feel free to email me at chuckfinca at gmail dot com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learnings",
    "section": "",
    "text": "Creating an Evaluation Framework\n\n\n\n\n\n\nevaluations\n\n\nllms\n\n\n\nBuilding out a framework that takes a model and a benchmark as input and returns the score as output.\n\n\n\n\n\nAug 31, 2024\n\n\n\n\n\n\nNo matching items"
  }
]